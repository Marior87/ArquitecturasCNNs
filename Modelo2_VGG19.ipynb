{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue;\">Redes Neuronales Convolucionales para la detección de puntos anatómicos en imágenes cefálicas laterales</h1>\n",
    "\n",
    "<p>En este proyecto busco utilizar algunas arquitecturas de redes neuronales convolucionales para determinar cual de ellas se adecúa mejor a la detección de puntos anatómicos en imágenes radiográficas cefálicas laterales, intentaré hacerlo lo mas sencillo posible.</p>\n",
    "\n",
    "<h2 style=\"color:red;\">Data Utilizada</h2>\n",
    "\n",
    "<p>Para realizar esta investigación, voy a usar una data disponible de forma gratuita en <a href=\"http://www-o.ntust.edu.tw/~cweiwang/ISBI2015/challenge1/index.html\">Dataset</a>, de la misma, solo utilice la data con labels \"senior\".</p>\n",
    "\n",
    "<h3>Breve Descripción de la data</h3>\n",
    "\n",
    "<p>Los datos a utilizar consisten en 400 imágenes de imágenes radiográficas cefálicas laterales (escala de grises) junto con un vector que indica la posición de 19 puntos anatómicos, etiquetados por un especialista senior en el área (ver link del dataset para mayor detalle).</p>\n",
    "\n",
    "<h2 style=\"color:red;\">Preprocesamiento y Data Augmentation</h2>\n",
    "\n",
    "<p>Para hacer el modelo más general, las imágenes fueron preprocesadas para cumplir con:</p>\n",
    "\n",
    "* Imágenes cuadradas de 128x128 pixeles.\n",
    "* Imágenes normalizadas (rango de intensidad de pixeles de -1 a 1).\n",
    "* Para tener mayor cantidad de datos y evitar overfit, se aumentó la data al hacer flip horizontal y vertical, para un total de 1200 imágenes con sus labels (400 originales + 400 flip horizontal + 400 flip vertical).\n",
    "* Igualmente, para evitar overfit, las imágenes están organizadas en 1 original - 1 flip h - 1 flip v. Nota: Es cierto que lo ideal sería mezclar todo, sin embargo, las imágenes \"flipeadas\" distan tanto de la original que no creo que el efecto sea tan evidente, de todas formas es un punto a explorar.\n",
    "* El training set consta de 900 imágenes, mientras que el test set de 300 imágenes.\n",
    "\n",
    "\n",
    "<p>Tomemos una imagen de ejemplo junto con sus labels:</p>\n",
    "\n",
    "<img src=\"imagen1.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "<p>Naturalmente, la imagen no es nítida debido a que tiene solo 128x128 píxeles de resolución. Los puntos rojos representan los puntos anatómicos etiquetados por el especialista.</p>\n",
    "\n",
    "<h2 style=\"color:red;\">Entorno de Ejecución</h2>\n",
    "\n",
    "<p>Debido a diversas limitantes, la mejor opción que tengo disponible es, sin dudas, usando el entorno acelerado por GPU en Google Colab (no se imaginan cuanto lo agardezco), la forma de setearlo la obtuve de este post de <a href=\"https://www.kdnuggets.com/2018/02/google-colab-free-gpu-tutorial-tensorflow-keras-pytorch.html\">KDnuggets.</a> Recomiendo ampliamente darle una hojeada para entender mejor como funciona.<br><br>\n",
    "    Para hacer mas sencillo el código, se utilizará Keras con backend TensorFlow, lo cual hace el modelo muy fácil de armar, depurar y leer. La data está almacenada en mi Google Drive, la compartiré en cuanto esté seguro que tengo permiso de hacerlo, igual la data original es accesible en el link de la sección \"Data Utilizada\".</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Estableciendo la conexión con nuestro Google Drive</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>El código anterior tendrá dos puntos en los que nos indica un link al cual debemos acceder para iniciar nuestra sesión en Google y darle autorización a nuestra aplicación para acceder al Drive. Es tan sencillo como copiar los códigos de autenticación en los campos indicados.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Procedemos a montar nuestro Drive a través de la referencia \"drive\"</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Ahora, verificamos que nuestro entorno en Google Colab tiene configurado el entorno acelerado por GPU, lo cual nos debería arrojar <b>['/device:GPU:0']</b> en caso afirmativo.</p>\n",
    "\n",
    "<p><b>Nota: </b> El programa debe funcionar aun cuando no se encuentre acelerado por GPU, sin embargo, la diferencia en rapidez (y por tanto en viabilidad de la investigación) es muy considerable en mi caso, siendo alrededor de 20 a 30 veces más rápido, sin contar el hecho de que el computador no queda \"esclavizado\" por el uso exhaustivo de CPU y Memoria RAM.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importando librerías necesarias y cargando el training y test set</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q keras #En caso de que no lo esté\n",
    "\n",
    "import pickle\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Convolution2D, MaxPooling2D, concatenate\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.optimizers\n",
    "from keras.utils import plot_model\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Cargamos la data desde nuestro Drive en un diccionario con dos índices princiaples:</p>\n",
    "\n",
    "* Diccionario['Img'] para obtener las imágenes (matriz de intensidad de grises).\n",
    "* Diccionario['Lbl'] para obtener los labels.\n",
    "\n",
    "<p>Es importante notar que índices iguales corresponden al par Imagen-Label respectivo, esto es, Diccionario['Img'][15] es una imagen cuyo label está guardado en Diccionario['Lbl'][15].</p>\n",
    "    \n",
    "<p>La ruta dentro de mi sesión de Google Drive para obtener la data a evaluar es \"drive/ColabRuns/ImagenesFlipLabelsSenior128.pickle\". Como se puede notar, es un archivo tipo 'pickle', para facilitar su lectura y por ser ligero en espacio que ocupa.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diccionario = pickle.load(open(\"drive/ColabRuns/ImagenesFlipLabelsSenior128.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Definimos nuestras variables de training y test según lo especificado anteriormente<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(Diccionario['Img'][0:900]).reshape(900,128,128,1).astype('float32')\n",
    "Y_train=np.array(Diccionario['Lbl'][0:900]).reshape(900,38).astype('float32')\n",
    "\n",
    "X_test=np.array(Diccionario['Img'][900:1200]).reshape(300,128,128,1).astype('float32')\n",
    "Y_test=np.array(Diccionario['Lbl'][900:1200]).reshape(300,38).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Algunas notas sobre el reshape y el astype:</p>\n",
    "* reshape(900,128,128,1) corresponde a 900 imágenes, de 128 x 128 pixeles, de 1 canal cada una.\n",
    "* reshape(900,38) corresponde a 900 labels, cada uno de 38 elementos (19 pares de coordenadas).\n",
    "* astype('float32') es un tipo de dato aceptado por TensorFlow y Keras, cuando se guardó la data en un archivo 'pickle', los datos pueden no ser devueltos en un formato aceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Una vez cumplido lo explicado anteriormente, llegamos al punto en que debemos definir la arquitectura de red neuronal convolucional en si, en este caso, usaremos una arquitectura del tipo \"AlexNet\", que no es mas que una arquitectura secuencial, pero que se reconoce como el punto de inflexión que hizo posible el actual desarrollo y uso de las CNNs en el área de visión por computador.</p>\n",
    "\n",
    "<p><b>Nota: </b>Las demás arquitecturas a evaluar serán exactamente iguales hasta aquí.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Definición del Modelo: VGG19</h1>\n",
    "\n",
    "<p>El modelo está basado en el trabajo de K. Simonyan y A. Zisserman:</p>\n",
    "<br><b>\n",
    "Very Deep Convolutional Networks for Large-Scale Image Recognition. K. Simonyan, A. Zisserman\n",
    "arXiv:1409.1556\n",
    "</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = Input(shape=(128,128,1))\n",
    "\n",
    "Conv1 = Convolution2D(64, kernel_size=3, strides=1, activation='relu', padding='same')(input1)\n",
    "Conv2 = Convolution2D(64, kernel_size=3, strides=1, activation='relu', padding='same')(Conv1)\n",
    "Maxp1 = MaxPooling2D(pool_size=(2,2))(Conv2)\n",
    "\n",
    "Conv3 = Convolution2D(128, kernel_size=3, strides=1, activation='relu', padding='same')(Maxp1)\n",
    "Conv4 = Convolution2D(128, kernel_size=3, strides=1, activation='relu', padding='same')(Conv3)\n",
    "Maxp2 = MaxPooling2D(pool_size=(2,2))(Conv4)\n",
    "\n",
    "Conv5 = Convolution2D(256, kernel_size=3, strides=1, activation='relu', padding='same')(Maxp2)\n",
    "Conv6 = Convolution2D(256, kernel_size=3, strides=1, activation='relu', padding='same')(Conv5)\n",
    "Maxp3 = MaxPooling2D(pool_size=(2,2))(Conv6)\n",
    "\n",
    "Conv7 = Convolution2D(512, kernel_size=3, strides=1, activation='relu', padding='same')(Maxp3)\n",
    "Conv8 = Convolution2D(512, kernel_size=3, strides=1, activation='relu', padding='same')(Conv7)\n",
    "Conv9 = Convolution2D(512, kernel_size=3, strides=1, activation='relu', padding='same')(Conv8)\n",
    "Conv10 = Convolution2D(512, kernel_size=3, strides=1, activation='relu', padding='same')(Conv9)\n",
    "Maxp4 = MaxPooling2D(pool_size=(2,2))(Conv10)\n",
    "\n",
    "Conv11 = Convolution2D(512, kernel_size=3, strides=1, activation='relu', padding='same')(Maxp4)\n",
    "Conv12 = Convolution2D(512, kernel_size=3, strides=1, activation='relu', padding='same')(Conv11)\n",
    "Conv13= Convolution2D(512, kernel_size=3, strides=1, activation='relu', padding='same')(Conv12)\n",
    "Conv14 = Convolution2D(512, kernel_size=3, strides=1, activation='relu', padding='same')(Conv13)\n",
    "Maxp5 = MaxPooling2D(pool_size=(2,2))(Conv14)\n",
    "\n",
    "flat = Flatten()(Maxp5)\n",
    "\n",
    "FC1 = Dense(4096, activation='relu')(flat)\n",
    "DO1 = Dropout(0.5)(FC1)\n",
    "FC2 = Dense(4096, activation='relu')(DO1)\n",
    "FC3 = Dense(38, activation='relu')(FC2)\n",
    "\n",
    "\n",
    "model = Model(inputs=input1, outputs=FC3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Veamos algunas características del modelo:<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Resumen:</h3>\n",
    "\n",
    "\n",
    "<img src=\"ResumenVGG19.png\" alt=\"Drawing\" style=\"width: 750px;\"/>\n",
    "\n",
    "<h3>Grafo:</h3>\n",
    "\n",
    "<img src=\"modelVGG19.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Definamos ahora el optimizador, condiciones de parada (stopper) y métricas de corrida:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=keras.optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#Callbacks: csv_logger: Registro de la corrida. stopper: Condiciones de parada antes de las 500 iteraciones.\n",
    "csv_logger = keras.callbacks.CSVLogger('drive/training_Modelo1_VGG19.log')\n",
    "stopper = keras.callbacks.EarlyStopping(monitor='loss', min_delta=1e-08, patience=15, verbose=0, mode='auto')\n",
    "model.compile(loss='mean_squared_error',optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Ejecutemos el modelo:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, batch_size=50, nb_epoch=500, verbose=1,callbacks=[csv_logger,stopper])\n",
    "#Nota: El batch size es menor debido a que el modelo es demasiado pesado para manejar 128 imágenes a la vez\n",
    "#con los recursos con los que se cuenta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Evaluación del Test Set y visualización de resultados</h2>\n",
    "\n",
    "<p>Después de finalizada la corrida, procedemos a evaluar el modelo con el test set:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Loss del Test Set: \"+str(score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Loss del Test Set: 0.0021535399928689003 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>El modelo debe ser corrido varias veces para obtener un promedio de la pérdida real, aquí solo se muestra una vez.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para visualizar una imágen con los labels reales (rojo) vs los predichos (azules)\n",
    "def probador(start=1000):\n",
    "    vector = model.predict(np.array(Diccionario['Img'][start:start+1]).reshape(1,128,128,1).astype('float32'),batch_size=1,verbose=0)\n",
    "    plt.imshow(np.reshape(Diccionario['Img'][start:start+1][0],[128,128]),cmap='gray')\n",
    "    Y_Predicted = np.array(vector,dtype='float32').reshape([19,2])\n",
    "    return plt.scatter(x=Y_Predicted[:,0]*128,y=Y_Predicted[:,1]*128, c='b', s=4),plt.scatter(x=Diccionario['Lbl'][start:start+1][0].reshape([19,2])[:,0]*128,y=Diccionario['Lbl'][start:start+1][0].reshape([19,2])[:,1]*128, c='r', s=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Algunas imágenes de resultado, recordar que los puntos rojos son los labels \"reales\" y los azules los labels \"predichos\":</p>\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"ImagenesVGG19/1014.png\" style=\"width: 300px; float: left;\"/>\n",
    "\n",
    "<img src=\"ImagenesVGG19/1019.png\" style=\"width: 300px; float: left;\"/>\n",
    "\n",
    "<img src=\"ImagenesVGG19/1036.png\" style=\"width: 300px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Comentarios:</h4>\n",
    "\n",
    "<p>El modelo presenta menor desempeño que el AlexNet, cabe destacar que existen numerosas razones por las cuales esto puede suceder, en mi opinión se debe a ajustes en los hiperparámetros de aprendizaje, especialmente el learning rate y el algoritmo de optimización. En pruebas separadas que he hecho con arquitecturas propias, ADAM tiene mejor desempeño que SGD con momentum.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
