{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:blue;\">Redes Neuronales Convolucionales para la detección de puntos anatómicos en imágenes cefálicas laterales</h1>\n",
    "\n",
    "<p>En este proyecto busco utilizar algunas arquitecturas de redes neuronales convolucionales para determinar cual de ellas se adecúa mejor a la detección de puntos anatómicos en imágenes radiográficas cefálicas laterales, intentaré hacerlo lo mas sencillo posible.</p>\n",
    "\n",
    "<h2 style=\"color:red;\">Data Utilizada</h2>\n",
    "\n",
    "<p>Para realizar esta investigación, voy a usar una data disponible de forma gratuita en <a href=\"http://www-o.ntust.edu.tw/~cweiwang/ISBI2015/challenge1/index.html\">Dataset</a>, de la misma, solo utilice la data con labels \"senior\".</p>\n",
    "\n",
    "<h3>Breve Descripción de la data</h3>\n",
    "\n",
    "<p>Los datos a utilizar consisten en 400 imágenes de imágenes radiográficas cefálicas laterales (escala de grises) junto con un vector que indica la posición de 19 puntos anatómicos, etiquetados por un especialista senior en el área (ver link del dataset para mayor detalle).</p>\n",
    "\n",
    "<h2 style=\"color:red;\">Preprocesamiento y Data Augmentation</h2>\n",
    "\n",
    "<p>Para hacer el modelo más general, las imágenes fueron preprocesadas para cumplir con:</p>\n",
    "\n",
    "* Imágenes cuadradas de 128x128 pixeles.\n",
    "* Imágenes normalizadas (rango de intensidad de pixeles de -1 a 1).\n",
    "* Para tener mayor cantidad de datos y evitar overfit, se aumentó la data al hacer flip horizontal y vertical, para un total de 1200 imágenes con sus labels (400 originales + 400 flip horizontal + 400 flip vertical).\n",
    "* Igualmente, para evitar overfit, las imágenes están organizadas en 1 original - 1 flip h - 1 flip v. Nota: Es cierto que lo ideal sería mezclar todo, sin embargo, las imágenes \"flipeadas\" distan tanto de la original que no creo que el efecto sea tan evidente, de todas formas es un punto a explorar.\n",
    "* El training set consta de 900 imágenes, mientras que el test set de 300 imágenes.\n",
    "\n",
    "<h2 style=\"color:red;\">Entorno de Ejecución</h2>\n",
    "\n",
    "<p>Debido a diversas limitantes, la mejor opción que tengo disponible es, sin dudas, usando el entorno acelerado por GPU en Google Colab (no se imaginan cuanto lo agardezco), la forma de setearlo la obtuve de este post de <a href=\"https://www.kdnuggets.com/2018/02/google-colab-free-gpu-tutorial-tensorflow-keras-pytorch.html\">KDnuggets.</a> Recomiendo ampliamente darle una hojeada para entender mejor como funciona.<br><br>\n",
    "    Para hacer mas sencillo el código, se utilizará Keras con backend TensorFlow, lo cual hace el modelo muy fácil de armar, depurar y leer. La data está almacenada en mi Google Drive, la compartiré en cuanto esté seguro que tengo permiso de hacerlo, igual la data original es accesible en el link de la sección \"Data Utilizada\".</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Estableciendo la conexión con nuestro Google Drive</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>El código anterior tendrá dos puntos en los que nos indica un link al cual debemos acceder para iniciar nuestra sesión en Google y darle autorización a nuestra aplicación para acceder al Drive. Es tan sencillo como copiar los códigos de autenticación en los campos indicados.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Procedemos a montar nuestro Drive a través de la referencia \"drive\"</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Ahora, verificamos que nuestro entorno en Google Colab tiene configurado el entorno acelerado por GPU, lo cual nos debería arrojar <b>['/device:GPU:0']</b> en caso afirmativo.</p>\n",
    "\n",
    "<p><b>Nota: </b> El programa debe funcionar aun cuando no se encuentre acelerado por GPU, sin embargo, la diferencia en rapidez (y por tanto en viabilidad de la investigación) es muy considerable en mi caso, siendo alrededor de 20 a 30 veces más rápido, sin contar el hecho de que el computador no queda \"esclavizado\" por el uso exhaustivo de CPU y Memoria RAM.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importando librerías necesarias y cargando el training y test set</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q keras #En caso de que no lo esté\n",
    "\n",
    "import pickle\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Convolution2D, MaxPooling2D, concatenate\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.optimizers\n",
    "from keras.utils import plot_model\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Cargamos la data desde nuestro Drive en un diccionario con dos índices princiaples:</p>\n",
    "\n",
    "* Diccionario['Img'] para obtener las imágenes (matriz de intensidad de grises).\n",
    "* Diccionario['Lbl'] para obtener los labels.\n",
    "\n",
    "<p>Es importante notar que índices iguales corresponden al par Imagen-Label respectivo, esto es, Diccionario['Img'][15] es una imagen cuyo label está guardado en Diccionario['Lbl'][15].</p>\n",
    "    \n",
    "<p>La ruta dentro de mi sesión de Google Drive para obtener la data a evaluar es \"drive/ColabRuns/ImagenesFlipLabelsSenior128.pickle\". Como se puede notar, es un archivo tipo 'pickle', para facilitar su lectura y por ser ligero en espacio que ocupa.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diccionario = pickle.load(open(\"drive/ColabRuns/ImagenesFlipLabelsSenior128.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Definimos nuestras variables de training y test según lo especificado anteriormente<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(Diccionario['Img'][0:900]).reshape(900,128,128,1).astype('float32')\n",
    "Y_train=np.array(Diccionario['Lbl'][0:900]).reshape(900,38).astype('float32')\n",
    "\n",
    "X_test=np.array(Diccionario['Img'][900:1200]).reshape(300,128,128,1).astype('float32')\n",
    "Y_test=np.array(Diccionario['Lbl'][900:1200]).reshape(300,38).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Algunas notas sobre el reshape y el astype:</p>\n",
    "* reshape(900,128,128,1) corresponde a 900 imágenes, de 128 x 128 pixeles, de 1 canal cada una.\n",
    "* reshape(900,38) corresponde a 900 labels, cada uno de 38 elementos (19 pares de coordenadas).\n",
    "* astype('float32') es un tipo de dato aceptado por TensorFlow y Keras, cuando se guardó la data en un archivo 'pickle', los datos pueden no ser devueltos en un formato aceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Una vez cumplido lo explicado anteriormente, llegamos al punto en que debemos definir la arquitectura de red neuronal convolucional en si, en este caso, usaremos una arquitectura del tipo \"AlexNet\", que no es mas que una arquitectura secuencial, pero que se reconoce como el punto de inflexión que hizo posible el actual desarrollo y uso de las CNNs en el área de visión por computador.</p>\n",
    "\n",
    "<p><b>Nota: </b>Las demás arquitecturas a evaluar serán exactamente iguales hasta aquí.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
